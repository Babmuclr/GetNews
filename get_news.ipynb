{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib import request  # urllib.requestモジュールをインポート\n",
    "from bs4 import BeautifulSoup  # BeautifulSoupクラスをインポート\n",
    "import lxml\n",
    "from newspaper import Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-30\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt \n",
    "now = dt.datetime.utcnow()\n",
    "now -= dt.timedelta(days=1)\n",
    "year, month, day = now.year, now.month, now.day\n",
    "limit_date = str(year) + \"-\" + str(month) + \"-\" + str(day)\n",
    "date = str(year) + \"-\" + str(month)\n",
    "print(limit_date) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"https://news.google.com/rss/search?q=after:2021-11-23+inurl:finance.yahoo.com/news/&hl=en-US&gl=US&ceid=US:en\"\n",
    "# url = \"https://news.google.com/rss/search?q=after:2021-11-23+inurl:finance.yahoo.com/news/&hl=en-US&gl=US&ceid=US:en\"\n",
    "# response = request.urlopen(url)\n",
    "# soup = BeautifulSoup(response,\"xml\")\n",
    "# response.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_names = [\" - Reuters\", \" - Bloomberg\", \" - CNBC\", \" - TheStreet\", \" - Fox Business\"]\n",
    "website_names_2 = [\"Reuters\", \"Bloomberg\", \"CNBC\", \"TheStreet\", \"Fox Business\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items(stock=\"Apple\", date=\"2021-11-24\", homepage=\"reuters.com\"):\n",
    "    stock = urllib.parse.quote(stock)\n",
    "    url = \"https://news.google.com/rss/search?q=\" + stock +  \"+after:\" + date + \"+inurl:\" + homepage + \"&hl=en-US&gl=US&ceid=US:en\"\n",
    "    response = request.urlopen(url)\n",
    "    soup = BeautifulSoup(response,\"xml\")\n",
    "    response.close()\n",
    "    elems = soup.find_all(\"item\")\n",
    "    return elems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./news_site.txt\") as f:\n",
    "    news_sites = f.readlines()\n",
    "news_sites = list(map(lambda x: x[:-1],news_sites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./stocks.txt\") as f:\n",
    "    stocks = f.readlines()\n",
    "stocks = list(map(lambda x: x[:-1],stocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in stocks:\n",
    "#     filename = \"./articles/stocks/\"+i\n",
    "#     os.mkdir(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import csv\n",
    "# for stock in stocks:\n",
    "#     item_size = 0\n",
    "#     filename = \"./articles/stocks/\" + stock + \"/\"\n",
    "#     # os.mkdir(filename)\n",
    "#     for news_site, website_name, website_name_2 in zip(news_sites, website_names, website_names_2 ):\n",
    "#         items = get_items(stock=stock, date=limit_date, homepage=news_site)\n",
    "#         item_size += len(items)\n",
    "#         for item in items[:3]:\n",
    "#             title = item.find(\"title\").getText()\n",
    "#             title = str(title.replace(website_name,\"\"))\n",
    "#             link = item.find(\"link\").getText()\n",
    "#             pubdate = item.find(\"pubDate\").getText()\n",
    "#             source = website_name_2\n",
    "#             data = [title,link,pubdate,source]\n",
    "#             with open(filename + date + \".csv\", \"a\", newline=\"\") as f:\n",
    "#                 writer = csv.writer(f)\n",
    "#                 writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "for news_site, website_name, website_name_2 in zip(news_sites, website_names, website_names_2 ):\n",
    "    items = get_items(stock=\"\", date=limit_date, homepage=news_site)\n",
    "    for item in items[:5]:\n",
    "        title = item.find(\"title\").getText()\n",
    "        title = str(title.replace(website_name,\"\"))\n",
    "        link = item.find(\"link\").getText()\n",
    "        pubdate = item.find(\"pubDate\").getText()\n",
    "        source = website_name_2\n",
    "        # data = [title,link,pubdate,source]\n",
    "        articles.append(\n",
    "            {\n",
    "                \"title\": title,\n",
    "                \"pubDate\": pubdate,\n",
    "                \"source\": source,\n",
    "                \"link\": link,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "\n",
    "cred = credentials.Certificate(\"./serviceAccountKey.json\")\n",
    "firebase_admin.initialize_app(cred)\n",
    "db = firestore.client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import urllib.parse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_time = 1\n",
    "try_max_count = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_translated_text(from_lang, to_lang, from_text):\n",
    "     \n",
    "    # urlencode\n",
    "    from_text = urllib.parse.quote(from_text)\n",
    "     \n",
    "    #　url作成\n",
    "    url = 'https://www.deepl.com/translator#' + from_lang +'/' + to_lang + '/' + from_text\n",
    "     \n",
    "    #　ヘッドレスモードでブラウザを起動\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "     \n",
    "    # ブラウザーを起動\n",
    "    driver = webdriver.Chrome('./chromedriver', options=options)\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(10)  # 見つからないときは、10秒まで待つ\n",
    " \n",
    " \n",
    "    for i in range(try_max_count):\n",
    "         \n",
    "        # 指定時間待つ\n",
    "        time.sleep(sleep_time)  \n",
    "        html = driver.page_source\n",
    "        to_text = get_text_from_page_source(html)\n",
    "         \n",
    "        try_count = i + 1\n",
    "     \n",
    "        if to_text:\n",
    "            wait_time =  sleep_time * try_count\n",
    "            print(str(wait_time) + \"秒\")\n",
    "             \n",
    "            # アクセス修了\n",
    "            break\n",
    "             \n",
    "    # ブラウザ停止\n",
    "    driver.quit()\n",
    "     \n",
    "    return to_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_page_source(html):\n",
    "    soup = BeautifulSoup(html, features='lxml')\n",
    "    target_elem = soup.find(class_=\"lmt__translations_as_text__text_btn\")\n",
    "    text = target_elem.text\n",
    "     \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omicron variant detected in more countries as scientists race to find answers Australia\n",
      "Onyx coal-fired power plant to shut with Dutch government support\n",
      "EXCLUSIVE Shell eyes return to Libya with oil, gas, solar investments\n",
      "Macquarie, France's Engie to build Australian battery with Fluence\n",
      "Amazon's cloud unit launches new chips to take on Intel, Nvidia\n",
      "Software AG Said to Explore Sale of $3 Billion Tech Company\n",
      "Meta (FB) Executive and Diem Co-Creator David Marcus Is Leaving Company\n",
      "Omicron Variant in South Africa: Kids Under 2 Are 10% of Hospital Cases\n",
      "Euro-Area Inflation Hits Record 4.9%, Beating All Forecasts\n",
      "Pfizer Has Best Month in 30 Years After Leading Vaccine Race\n",
      "Cramer suggests putting some cash to work after Tuesday's decline, 'it's too late to sell'\n",
      "Dow drops 650 points on growing omicron fears, Powell taper comments\n",
      "Asia-Pacific stocks mostly fall as investors watch omicron Covid variant; oil prices rebound\n",
      "S&P 500 bounces 1.3% from Friday's rout after Biden says there's no need for Covid omicron lockdowns\n",
      "Cyber Monday online sales drop 1.4% from last year to $10.7 billion, falling for the first time ever\n",
      "Housing Prices Cool as Market Slows\n",
      "6 Biggest Money Challenges Facing Gen Z and Millennials\n",
      "Cyber Monday Cost Buyers 13.9% More This Year\n",
      "Square Lifted to Neutral, Twitter Reiterated Buy at Bank of America\n",
      "Moderna, Regeneron Omicron Warnings Drag US Oil Prices To September Lows\n",
      "Who is Parag Agrawal, the new CEO of Twitter?\n",
      "Goldman Sachs CEO warns New York City over high taxes\n",
      "Salesforce's Benioff gets co-CEO\n",
      "Jury awards woman Walmart accused of shoplifting $2.1 million\n",
      "Mattel CEO feels ‘encouraged’ Biden will untangle supply chain crisis amid holiday season\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titles = \"\"\n",
    "for article in articles:\n",
    "    article = article[\"title\"].replace(\"/\",\"\")\n",
    "    titles += (article+\"\\n\")\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5秒\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "     \n",
    "    from_lang = 'en'\n",
    "    to_lang = 'ja'\n",
    "    #from_text = '提供された翻訳の正確性やサービスの利用可能性について、一切の責任を負いません。'\n",
    "    from_text = titles\n",
    " \n",
    "    # 翻訳\n",
    "    to_text = get_translated_text(from_lang, to_lang, from_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['オミクロン・バリアントがより多くの国で検出され、科学者たちが答えを探している オーストラリア', 'オニキス石炭火力発電所、オランダ政府の支援で閉鎖へ', '石油、ガス、太陽光発電への投資でリビアへの復帰を目指すシェル', 'マッコーリーと仏Engie、Fluenceでオーストラリアのバッテリーを構築', 'アマゾンのクラウド部門、インテルやNvidiaに対抗する新チップを発表', 'Software AG、30億ドル規模のハイテク企業の売却を検討していると言われる', 'Meta (FB)社の役員でDiemの共同開発者であるDavid Marcus氏が会社を去ることになりました。', '南アフリカでオミクロン・バリアントが発生。2歳未満の子供が病院の10％を占める', 'ユーロ圏のインフレ率、予想を上回る4.9％を記録', 'ワクチン競争をリードしてきたPfizer社が30年ぶりに最高の月を迎える', 'Cramer氏、火曜の下落で現金を投入することを提案「売るには遅すぎる', 'オミクロン恐怖症の拡大とパウエルのテーパリング発言でダウは650ポイント下落', 'アジア太平洋地域の株式は、オミクロンのコビットの変動を見守る投資家の影響でほとんどが下落、原油価格は反発', 'S&P 500、BidenがCovidのロックダウンは必要ないと発言したことで、金曜日の暴落から1.3%の反発。', 'サイバーマンデーのオンライン売上高は昨年比1.4％減の107億ドル、史上初のマイナスとなる', '市場の低迷で住宅価格が冷え込む', 'Z世代とミレニアル世代が直面する6つの大きなお金の問題', '今年のサイバーマンデーは購入者に13.9％の負担を強いた', 'バンク・オブ・アメリカ、Squareを中立に、Twitterを買いに変更', 'モデナ、リジェネロン・オミクロンの警告が米国の原油価格を9月の最安値に引きずり込む', 'Twitterの新CEO、Parag Agrawal氏とは？', 'ゴールドマン・サックスのCEOが高額な税金でニューヨーク市に警告', 'セールスフォースのベニオフ氏、共同CEOに就任', '陪審員は万引きで訴えられたWalmartの女性に210万ドルを与える', 'マテル社CEO、バイデン氏がホリデーシーズン中のサプライチェーン危機を解いてくれると「心強い」と感じる']\n"
     ]
    }
   ],
   "source": [
    "japanese_title = list(to_text.split(\"\\n\"))\n",
    "print(japanese_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(japanese_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(articles)):\n",
    "    articles[i][\"japanese_title\"] = japanese_title[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Omicron variant detected in more countries as scientists race to find answers Australia',\n",
       " 'pubDate': 'Sun, 28 Nov 2021 23:02:00 GMT',\n",
       " 'source': 'Reuters',\n",
       " 'link': 'https://www.reuters.com/world/new-coronavirus-variant-omicron-keeps-spreading-australia-detects-cases-2021-11-28/',\n",
       " 'japanese_title': 'オミクロン・バリアントがより多くの国で検出され、科学者たちが答えを探している オーストラリア'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for article in articles:\n",
    "    title = article[\"title\"]\n",
    "    hash_val = hashlib.sha224(title.encode()).hexdigest()\n",
    "    with open(\"./articles/importants/articles.txt\", \"r\") as f:\n",
    "        x = f.read()\n",
    "    hash_li = list(x.split(\"\\n\"))[:-1]\n",
    "    if hash_val not in hash_li:\n",
    "        with open(\"./articles/importants/articles.txt\", \"a\", newline=\"\") as f:\n",
    "            f.write(hash_val+\"\\n\")\n",
    "        doc_ref = db.collection(\"articles\").document()\n",
    "        doc_ref.set({\n",
    "            u\"title\": article[\"title\"],\n",
    "            u\"pubDate\": article[\"pubDate\"],\n",
    "            u\"link\": article[\"link\"],\n",
    "            u\"source\": article[\"source\"],\n",
    "            u\"title_ja\": article[\"japanese_title\"],  \n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
